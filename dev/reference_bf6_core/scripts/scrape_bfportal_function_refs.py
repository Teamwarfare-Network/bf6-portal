#!/usr/bin/env python3
"""
Scrape BF Portal API docs into local reference markdown files.

Targets:
- https://docs.bfportal.gg/santiago/scripting/mod
- https://docs.bfportal.gg/santiago/scripting/modlib
"""

from __future__ import annotations

import re
import sys
import urllib.error
import urllib.request
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Tuple


RAW_ROOT = "https://raw.githubusercontent.com/battlefield-portal-community/portal-docs/main/docs/santiago/scripting"
UA = {"User-Agent": "Mozilla/5.0"}


@dataclass(frozen=True)
class SectionConfig:
    name: str
    index_raw_url: str
    entry_raw_root: str
    entry_doc_root: str


SECTIONS = (
    SectionConfig(
        name="mod",
        index_raw_url=f"{RAW_ROOT}/mod/mod/_index.md",
        entry_raw_root=f"{RAW_ROOT}/mod/mod",
        entry_doc_root="https://docs.bfportal.gg/santiago/scripting/mod/mod",
    ),
    SectionConfig(
        name="modlib",
        index_raw_url=f"{RAW_ROOT}/modlib/_index.md",
        entry_raw_root=f"{RAW_ROOT}/modlib",
        entry_doc_root="https://docs.bfportal.gg/santiago/scripting/modlib",
    ),
)


def fetch_text(url: str) -> str:
    req = urllib.request.Request(url, headers=UA)
    with urllib.request.urlopen(req, timeout=60) as resp:
        return resp.read().decode("utf-8", errors="replace")


def unescape_markdown(text: str) -> str:
    # Convert escaped markdown punctuation into plain text.
    return re.sub(r"\\([\\`*_{}\[\]()#+\-.!])", r"\1", text)


def slugify(text: str) -> str:
    normalized = text.strip().lower()
    normalized = normalized.replace("&", "and")
    normalized = re.sub(r"[^a-z0-9]+", "-", normalized)
    normalized = re.sub(r"-{2,}", "-", normalized).strip("-")
    return normalized or "category"


def safe_filename(text: str) -> str:
    cleaned = re.sub(r'[<>:"/\\|?*]', "_", text.strip())
    cleaned = cleaned.rstrip(". ")
    return cleaned or "item"


def parse_index_categories(index_md: str) -> Dict[str, List[Tuple[str, str]]]:
    """
    Return category -> [(display_name, rel_path), ...] parsed from markdown H2 blocks.
    """
    categories: Dict[str, List[Tuple[str, str]]] = {}
    blocks = re.findall(
        r"^## ([^\n]+)\s*\n(.*?)(?=^## |\Z)",
        index_md,
        flags=re.MULTILINE | re.DOTALL,
    )

    for heading, body in blocks:
        rows = re.findall(r"^- \[([^\]]+)\]\(([^)]+)\)", body, flags=re.MULTILINE)
        cleaned_rows: List[Tuple[str, str]] = []
        for name, rel_path in rows:
            if rel_path.endswith("/_index.md"):
                cleaned_rows.append((unescape_markdown(name), rel_path))
        if cleaned_rows:
            categories[heading.strip()] = cleaned_rows

    return categories


def trim_generated_wrapper(md: str) -> str:
    # Drop YAML frontmatter and typedoc breadcrumb wrapper; keep from first H1 onward.
    if md.startswith("---"):
        fm_end = md.find("\n---", 3)
        if fm_end != -1:
            md = md[fm_end + 4 :]

    first_h1 = md.find("\n# ")
    if first_h1 != -1:
        md = md[first_h1 + 1 :]

    return md.strip() + "\n"


def write_entry_file(
    output_path: Path,
    docs_url: str,
    raw_url: str,
    content: str,
    generated_at: str,
) -> None:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    payload = (
        f"<!-- Generated by scripts/scrape_bfportal_function_refs.py at {generated_at} -->\n"
        f"<!-- Source docs page: {docs_url} -->\n"
        f"<!-- Source raw markdown: {raw_url} -->\n\n"
        f"{content}"
    )
    output_path.write_text(payload, encoding="utf-8")


def write_category_index(
    section_dir: Path,
    section: SectionConfig,
    category_heading: str,
    category_slug: str,
    rows: List[Tuple[str, str, str, str]],
    generated_at: str,
) -> None:
    section_dir.mkdir(parents=True, exist_ok=True)
    lines = [
        f"# {section.name} {category_heading}",
        "",
        f"- Generated at (UTC): `{generated_at}`",
        f"- Source index (raw): `{section.index_raw_url}`",
        f"- Source docs base: `{section.entry_doc_root}`",
        f"- Category: `{category_heading}`",
        f"- Total entries: `{len(rows)}`",
        "",
        "## Files",
        "",
    ]
    for display_name, rel_path, entry_key, file_name in rows:
        docs_url = f"{section.entry_doc_root}/{entry_key}"
        lines.append(
            f"- [{display_name}]({category_slug}/{file_name}.md) | `{rel_path}` | `{docs_url}`"
        )

    lines.append("")
    (section_dir / f"00-{category_slug}-index.md").write_text(
        "\n".join(lines), encoding="utf-8"
    )


def write_section_summary(
    section_dir: Path,
    section: SectionConfig,
    category_counts: List[Tuple[str, str, int]],
    generated_at: str,
) -> None:
    lines = [
        f"# {section.name} API Index",
        "",
        f"- Generated at (UTC): `{generated_at}`",
        f"- Source index (raw): `{section.index_raw_url}`",
        f"- Source docs base: `{section.entry_doc_root}`",
        "",
        "## Categories",
        "",
    ]
    for heading, slug, count in category_counts:
        lines.append(f"- [{heading}](00-{slug}-index.md) | `{count}` entries")
    lines.append("")
    (section_dir / "00-api-index.md").write_text("\n".join(lines), encoding="utf-8")


def write_root_summary(
    reference_root: Path,
    section_totals: List[Tuple[str, int, int]],
    generated_at: str,
) -> None:
    lines = [
        "# BF6 Core API Reference",
        "",
        f"- Generated at (UTC): `{generated_at}`",
        "- Sources:",
        "  - `https://docs.bfportal.gg/santiago/scripting/mod`",
        "  - `https://docs.bfportal.gg/santiago/scripting/modlib`",
        "",
        "## Sections",
        "",
    ]
    for section_name, category_count, entry_count in section_totals:
        lines.append(
            f"- [{section_name}]({section_name}/00-api-index.md) | `{category_count}` categories | `{entry_count}` entries"
        )
    lines.append("")
    (reference_root / "00-api-reference-index.md").write_text("\n".join(lines), encoding="utf-8")


def prune_stale_files(category_dir: Path, expected_files: set[str]) -> int:
    removed = 0
    if not category_dir.exists():
        return removed
    for path in category_dir.glob("*.md"):
        if path.name not in expected_files:
            path.unlink()
            removed += 1
    return removed


def run() -> int:
    script_dir = Path(__file__).resolve().parent
    reference_root = script_dir.parent
    generated_at = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    total_written = 0
    total_pruned = 0
    failures: list[str] = []
    section_totals: List[Tuple[str, int, int]] = []

    for section in SECTIONS:
        section_dir = reference_root / section.name

        try:
            index_md = fetch_text(section.index_raw_url)
        except urllib.error.URLError as err:
            failures.append(f"[{section.name}] failed to fetch index: {err}")
            continue

        categories = parse_index_categories(index_md)
        if not categories:
            failures.append(f"[{section.name}] no categories found in index")
            continue

        category_counts: List[Tuple[str, str, int]] = []
        section_entry_count = 0
        for category_heading, rows in categories.items():
            category_slug = slugify(category_heading)
            rows_for_index: List[Tuple[str, str, str, str]] = []
            category_dir = section_dir / category_slug
            expected_files: set[str] = set()

            for display_name, rel_path in rows:
                entry_key = rel_path[: -len("/_index.md")]
                raw_url = f"{section.entry_raw_root}/{rel_path}"
                docs_url = f"{section.entry_doc_root}/{entry_key}"
                file_name = safe_filename(entry_key)
                output_path = category_dir / f"{file_name}.md"
                rows_for_index.append((display_name, rel_path, entry_key, file_name))
                expected_files.add(f"{file_name}.md")

                try:
                    raw_md = fetch_text(raw_url)
                    content = trim_generated_wrapper(raw_md)
                    write_entry_file(
                        output_path=output_path,
                        docs_url=docs_url,
                        raw_url=raw_url,
                        content=content,
                        generated_at=generated_at,
                    )
                    total_written += 1
                    section_entry_count += 1
                except urllib.error.URLError as err:
                    failures.append(f"[{section.name}/{category_heading}] {display_name}: {err}")
                except OSError as err:
                    failures.append(f"[{section.name}/{category_heading}] {display_name}: {err}")

            total_pruned += prune_stale_files(category_dir, expected_files)
            write_category_index(
                section_dir=section_dir,
                section=section,
                category_heading=category_heading,
                category_slug=category_slug,
                rows=rows_for_index,
                generated_at=generated_at,
            )
            category_counts.append((category_heading, category_slug, len(rows_for_index)))

        write_section_summary(
            section_dir=section_dir,
            section=section,
            category_counts=category_counts,
            generated_at=generated_at,
        )
        section_totals.append((section.name, len(category_counts), section_entry_count))

    write_root_summary(
        reference_root=reference_root,
        section_totals=section_totals,
        generated_at=generated_at,
    )

    print(f"Wrote {total_written} API reference files.")
    print(f"Pruned {total_pruned} stale files.")
    for section_name, category_count, entry_count in section_totals:
        print(
            f"- {section_name}: {entry_count} files across {category_count} categories"
        )
    if failures:
        print("Failures:")
        for item in failures:
            print(f"- {item}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(run())
